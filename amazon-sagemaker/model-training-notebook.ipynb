{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train and Deploy a Neural Collaborative Filtering Model\n",
    "In this notebook, you will execute code blocks to\n",
    "1. inspect the training script ncf.py\n",
    "2. train a model using Tensorflow Estimator\n",
    "3. deploy and host the trained model as an endpoint using Amazon SageMaker Hosting Services\n",
    "4. perform batch inference by calling the model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n",
      "9724\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In the last notebook (data-preparation-notebook.ipynb), we stored two variables.\n",
    "# Let's restore those variables here. These variables are inputs for the model training process.\n",
    "\n",
    "%store -r n_user\n",
    "%store -r n_item\n",
    "\n",
    "print(n_user)\n",
    "print(n_item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# import requirements\n",
    "import os\n",
    "import json\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# get current SageMaker session's execution role and default bucket name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"execution role ARN:\", role)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "print(\"default bucket name:\", bucket_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# specify the location of the training data\n",
    "training_data_uri = os.path.join(f's3://{bucket_name}', 'data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m\"\"\"\u001B[39;49;00m\r\n",
      "\u001B[33m\u001B[39;49;00m\r\n",
      "\u001B[33m Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001B[39;49;00m\r\n",
      "\u001B[33m SPDX-License-Identifier: MIT-0\u001B[39;49;00m\r\n",
      "\u001B[33m\u001B[39;49;00m\r\n",
      "\u001B[33m Permission is hereby granted, free of charge, to any person obtaining a copy of this\u001B[39;49;00m\r\n",
      "\u001B[33m software and associated documentation files (the \"Software\"), to deal in the Software\u001B[39;49;00m\r\n",
      "\u001B[33m without restriction, including without limitation the rights to use, copy, modify,\u001B[39;49;00m\r\n",
      "\u001B[33m merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\u001B[39;49;00m\r\n",
      "\u001B[33m permit persons to whom the Software is furnished to do so.\u001B[39;49;00m\r\n",
      "\u001B[33m\u001B[39;49;00m\r\n",
      "\u001B[33m THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\u001B[39;49;00m\r\n",
      "\u001B[33m INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\u001B[39;49;00m\r\n",
      "\u001B[33m PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\u001B[39;49;00m\r\n",
      "\u001B[33m HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\u001B[39;49;00m\r\n",
      "\u001B[33m OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\u001B[39;49;00m\r\n",
      "\u001B[33m SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u001B[39;49;00m\r\n",
      "\u001B[33m\u001B[39;49;00m\r\n",
      "\u001B[33m\"\"\"\u001B[39;49;00m\r\n",
      "\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mtensorflow\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m \u001B[04m\u001B[36mtf\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36margparse\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mos\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mnumpy\u001B[39;49;00m \u001B[34mas\u001B[39;49;00m \u001B[04m\u001B[36mnp\u001B[39;49;00m\r\n",
      "\u001B[34mimport\u001B[39;49;00m \u001B[04m\u001B[36mjson\u001B[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001B[37m# for data processing\u001B[39;49;00m\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_load_training_data\u001B[39;49;00m(base_dir):\r\n",
      "    \u001B[33m\"\"\" load training data \"\"\"\u001B[39;49;00m\r\n",
      "    df_train = np.load(os.path.join(base_dir, \u001B[33m'\u001B[39;49;00m\u001B[33mtrain.npy\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m))\r\n",
      "    user_train, item_train, y_train = np.split(np.transpose(df_train).flatten(), \u001B[34m3\u001B[39;49;00m)\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m user_train, item_train, y_train\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mbatch_generator\u001B[39;49;00m(x, y, batch_size, n_batch, shuffle, user_dim, item_dim):\r\n",
      "    \u001B[33m\"\"\" batch generator to supply data for training and testing \"\"\"\u001B[39;49;00m\r\n",
      "\r\n",
      "    user_df, item_df = x\r\n",
      "\r\n",
      "    counter = \u001B[34m0\u001B[39;49;00m\r\n",
      "    training_index = np.arange(user_df.shape[\u001B[34m0\u001B[39;49;00m])\r\n",
      "\r\n",
      "    \u001B[34mif\u001B[39;49;00m shuffle:\r\n",
      "        np.random.shuffle(training_index)\r\n",
      "\r\n",
      "    \u001B[34mwhile\u001B[39;49;00m \u001B[34mTrue\u001B[39;49;00m:\r\n",
      "        batch_index = training_index[batch_size * counter:batch_size * (counter + \u001B[34m1\u001B[39;49;00m)]\r\n",
      "        user_batch = tf.one_hot(user_df[batch_index], depth=user_dim)\r\n",
      "        item_batch = tf.one_hot(item_df[batch_index], depth=item_dim)\r\n",
      "        y_batch = y[batch_index]\r\n",
      "        counter += \u001B[34m1\u001B[39;49;00m\r\n",
      "        \u001B[34myield\u001B[39;49;00m [user_batch, item_batch], y_batch\r\n",
      "\r\n",
      "        \u001B[34mif\u001B[39;49;00m counter == n_batch:\r\n",
      "            \u001B[34mif\u001B[39;49;00m shuffle:\r\n",
      "                np.random.shuffle(training_index)\r\n",
      "            counter = \u001B[34m0\u001B[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001B[37m# network\u001B[39;49;00m\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_get_user_embedding_layers\u001B[39;49;00m(inputs, emb_dim):\r\n",
      "    \u001B[33m\"\"\" create user embeddings \"\"\"\u001B[39;49;00m\r\n",
      "    user_gmf_emb = tf.keras.layers.Dense(emb_dim, activation=\u001B[33m'\u001B[39;49;00m\u001B[33mrelu\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)(inputs)\r\n",
      "\r\n",
      "    user_mlp_emb = tf.keras.layers.Dense(emb_dim, activation=\u001B[33m'\u001B[39;49;00m\u001B[33mrelu\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)(inputs)\r\n",
      "\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m user_gmf_emb, user_mlp_emb\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_get_item_embedding_layers\u001B[39;49;00m(inputs, emb_dim):\r\n",
      "    \u001B[33m\"\"\" create item embeddings \"\"\"\u001B[39;49;00m\r\n",
      "    item_gmf_emb = tf.keras.layers.Dense(emb_dim, activation=\u001B[33m'\u001B[39;49;00m\u001B[33mrelu\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)(inputs)\r\n",
      "\r\n",
      "    item_mlp_emb = tf.keras.layers.Dense(emb_dim, activation=\u001B[33m'\u001B[39;49;00m\u001B[33mrelu\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)(inputs)\r\n",
      "\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m item_gmf_emb, item_mlp_emb\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_gmf\u001B[39;49;00m(user_emb, item_emb):\r\n",
      "    \u001B[33m\"\"\" general matrix factorization branch \"\"\"\u001B[39;49;00m\r\n",
      "    gmf_mat = tf.keras.layers.Multiply()([user_emb, item_emb])\r\n",
      "\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m gmf_mat\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_mlp\u001B[39;49;00m(user_emb, item_emb, dropout_rate):\r\n",
      "    \u001B[33m\"\"\" multi-layer perceptron branch \"\"\"\u001B[39;49;00m\r\n",
      "\r\n",
      "    \u001B[34mdef\u001B[39;49;00m \u001B[32madd_layer\u001B[39;49;00m(dim, input_layer, dropout_rate):\r\n",
      "        hidden_layer = tf.keras.layers.Dense(dim, activation=\u001B[33m'\u001B[39;49;00m\u001B[33mrelu\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)(input_layer)\r\n",
      "\r\n",
      "        \u001B[34mif\u001B[39;49;00m dropout_rate:\r\n",
      "            dropout_layer = tf.keras.layers.Dropout(dropout_rate)(hidden_layer)\r\n",
      "            \u001B[34mreturn\u001B[39;49;00m dropout_layer\r\n",
      "\r\n",
      "        \u001B[34mreturn\u001B[39;49;00m hidden_layer\r\n",
      "\r\n",
      "    concat_layer = tf.keras.layers.Concatenate()([user_emb, item_emb])\r\n",
      "\r\n",
      "    dropout_l1 = tf.keras.layers.Dropout(dropout_rate)(concat_layer)\r\n",
      "\r\n",
      "    dense_layer_1 = add_layer(\u001B[34m64\u001B[39;49;00m, dropout_l1, dropout_rate)\r\n",
      "\r\n",
      "    dense_layer_2 = add_layer(\u001B[34m32\u001B[39;49;00m, dense_layer_1, dropout_rate)\r\n",
      "\r\n",
      "    dense_layer_3 = add_layer(\u001B[34m16\u001B[39;49;00m, dense_layer_2, \u001B[34mNone\u001B[39;49;00m)\r\n",
      "\r\n",
      "    dense_layer_4 = add_layer(\u001B[34m8\u001B[39;49;00m, dense_layer_3, \u001B[34mNone\u001B[39;49;00m)\r\n",
      "\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m dense_layer_4\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_neuCF\u001B[39;49;00m(gmf, mlp, dropout_rate):\r\n",
      "    concat_layer = tf.keras.layers.Concatenate()([gmf, mlp])\r\n",
      "\r\n",
      "    output_layer = tf.keras.layers.Dense(\u001B[34m1\u001B[39;49;00m, activation=\u001B[33m'\u001B[39;49;00m\u001B[33msigmoid\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)(concat_layer)\r\n",
      "\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m output_layer\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mbuild_graph\u001B[39;49;00m(user_dim, item_dim, dropout_rate=\u001B[34m0.25\u001B[39;49;00m):\r\n",
      "    \u001B[33m\"\"\" neural collaborative filtering model \"\"\"\u001B[39;49;00m\r\n",
      "\r\n",
      "    user_input = tf.keras.Input(shape=(user_dim))\r\n",
      "    item_input = tf.keras.Input(shape=(item_dim))\r\n",
      "\r\n",
      "    \u001B[37m# create embedding layers\u001B[39;49;00m\r\n",
      "    user_gmf_emb, user_mlp_emb = _get_user_embedding_layers(user_input, \u001B[34m32\u001B[39;49;00m)\r\n",
      "    item_gmf_emb, item_mlp_emb = _get_item_embedding_layers(item_input, \u001B[34m32\u001B[39;49;00m)\r\n",
      "\r\n",
      "    \u001B[37m# general matrix factorization\u001B[39;49;00m\r\n",
      "    gmf = _gmf(user_gmf_emb, item_gmf_emb)\r\n",
      "\r\n",
      "    \u001B[37m# multi layer perceptron\u001B[39;49;00m\r\n",
      "    mlp = _mlp(user_mlp_emb, item_mlp_emb, dropout_rate)\r\n",
      "\r\n",
      "    \u001B[37m# output\u001B[39;49;00m\r\n",
      "    output = _neuCF(gmf, mlp, dropout_rate)\r\n",
      "\r\n",
      "    \u001B[37m# create the model\u001B[39;49;00m\r\n",
      "    model = tf.keras.Model(inputs=[user_input, item_input], outputs=output)\r\n",
      "\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m model\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32mmodel\u001B[39;49;00m(x_train, y_train, n_user, n_item, num_epoch, batch_size):\r\n",
      "    num_batch = np.ceil(x_train[\u001B[34m0\u001B[39;49;00m].shape[\u001B[34m0\u001B[39;49;00m] / batch_size)\r\n",
      "\r\n",
      "    \u001B[37m# build graph\u001B[39;49;00m\r\n",
      "    model = build_graph(n_user, n_item)\r\n",
      "\r\n",
      "    \u001B[37m# compile and train\u001B[39;49;00m\r\n",
      "    optimizer = tf.keras.optimizers.Adam(learning_rate=\u001B[34m1e-3\u001B[39;49;00m)\r\n",
      "\r\n",
      "    model.compile(optimizer=optimizer,\r\n",
      "                  loss=tf.keras.losses.BinaryCrossentropy(),\r\n",
      "                  metrics=[\u001B[33m'\u001B[39;49;00m\u001B[33maccuracy\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m])\r\n",
      "\r\n",
      "    model.fit_generator(\r\n",
      "        generator=batch_generator(\r\n",
      "            x=x_train, y=y_train,\r\n",
      "            batch_size=batch_size, n_batch=num_batch,\r\n",
      "            shuffle=\u001B[34mTrue\u001B[39;49;00m, user_dim=n_user, item_dim=n_item),\r\n",
      "        epochs=num_epoch,\r\n",
      "        steps_per_epoch=num_batch,\r\n",
      "        verbose=\u001B[34m2\u001B[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m model\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mdef\u001B[39;49;00m \u001B[32m_parse_args\u001B[39;49;00m():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--model_dir\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m)\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--sm-model-dir\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m, default=os.environ.get(\u001B[33m'\u001B[39;49;00m\u001B[33mSM_MODEL_DIR\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m))\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--train\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m, default=os.environ.get(\u001B[33m'\u001B[39;49;00m\u001B[33mSM_CHANNEL_TRAINING\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m))\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--hosts\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mlist\u001B[39;49;00m, default=json.loads(os.environ.get(\u001B[33m'\u001B[39;49;00m\u001B[33mSM_HOSTS\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)))\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--current-host\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mstr\u001B[39;49;00m, default=os.environ.get(\u001B[33m'\u001B[39;49;00m\u001B[33mSM_CURRENT_HOST\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m))\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--epochs\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m, default=\u001B[34m3\u001B[39;49;00m)\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--batch_size\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m, default=\u001B[34m256\u001B[39;49;00m)\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--n_user\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m)\r\n",
      "    parser.add_argument(\u001B[33m'\u001B[39;49;00m\u001B[33m--n_item\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[36mtype\u001B[39;49;00m=\u001B[36mint\u001B[39;49;00m)\r\n",
      "\r\n",
      "    \u001B[34mreturn\u001B[39;49;00m parser.parse_known_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001B[34mif\u001B[39;49;00m \u001B[31m__name__\u001B[39;49;00m == \u001B[33m\"\u001B[39;49;00m\u001B[33m__main__\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m:\r\n",
      "    args, unknown = _parse_args()\r\n",
      "\r\n",
      "    \u001B[37m# load data\u001B[39;49;00m\r\n",
      "    user_train, item_train, train_labels = _load_training_data(args.train)\r\n",
      "\r\n",
      "    \u001B[37m# build model\u001B[39;49;00m\r\n",
      "    ncf_model = model(\r\n",
      "        x_train=[user_train, item_train],\r\n",
      "        y_train=train_labels,\r\n",
      "        n_user=args.n_user,\r\n",
      "        n_item=args.n_item,\r\n",
      "        num_epoch=args.epochs,\r\n",
      "        batch_size=args.batch_size\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001B[34mif\u001B[39;49;00m args.current_host == args.hosts[\u001B[34m0\u001B[39;49;00m]:\r\n",
      "        \u001B[37m# save model to an S3 directory with version number '00000001'\u001B[39;49;00m\r\n",
      "        ncf_model.save(os.path.join(args.sm_model_dir, \u001B[33m'\u001B[39;49;00m\u001B[33m000000001\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m), \u001B[33m'\u001B[39;49;00m\u001B[33mneural_collaborative_filtering.h5\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "# inspect the training script using `pygmentize` magic\n",
    "!pygmentize 'ncf.py'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# specify training instance type and model hyperparameters\n",
    "# note that for the demo purpose, the number of epoch is set to 1\n",
    "\n",
    "num_of_instance = 1                 # number of instance to use for training\n",
    "instance_type = 'ml.c5.2xlarge'     # type of instance to use for training\n",
    "\n",
    "training_script = 'ncf.py'\n",
    "\n",
    "training_parameters = {\n",
    "    'epochs': 1,\n",
    "    'batch_size': 256,\n",
    "    'n_user': n_user,\n",
    "    'n_item': n_item\n",
    "}\n",
    "\n",
    "# training framework specs\n",
    "tensorflow_version = '2.1.0'\n",
    "python_version = 'py3'\n",
    "distributed_training_spec = {'parameter_server': {'enabled': True}}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# initiate the training job using Tensorflow estimator\n",
    "ncf_estimator = TensorFlow(\n",
    "    entry_point=training_script,\n",
    "    role=role,\n",
    "    train_instance_count=num_of_instance,\n",
    "    train_instance_type=instance_type,\n",
    "    framework_version=tensorflow_version,\n",
    "    py_version=python_version,\n",
    "    distributions=distributed_training_spec,\n",
    "    hyperparameters=training_parameters\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# kick off the training job\n",
    "ncf_estimator.fit(training_data_uri)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Deploy the Endpoint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# once the model is trained, we can deploy the model using Amazon SageMaker Hosting Services\n",
    "# Here we deploy the model using one ml.c5.xlarge instance as a tensorflow-serving endpoint\n",
    "# This enables us to invoke the endpoint like how we use Tensorflow serving\n",
    "# Read more about Tensorflow serving using the link below\n",
    "# https://www.tensorflow.org/tfx/tutorials/serving/rest_simple\n",
    "\n",
    "endpoint_name = 'neural-collaborative-filtering-model-demo'\n",
    "\n",
    "predictor = ncf_estimator.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.c5.xlarge',\n",
    "                                 endpoint_type='tensorflow-serving',\n",
    "                                 endpoint_name=endpoint_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Invoke"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To use the endpoint in another notebook, we can initiate a predictor object as follows\n",
    "from sagemaker.tensorflow import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(endpoint_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to read testing data\n",
    "def _load_testing_data(base_dir):\n",
    "    \"\"\" load testing data \"\"\"\n",
    "    df_test = np.load(os.path.join(base_dir, 'test.npy'))\n",
    "    user_test, item_test, y_test = np.split(np.transpose(df_test).flatten(), 3)\n",
    "    return user_test, item_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read testing data from local\n",
    "user_test, item_test, test_labels = _load_testing_data('./ml-latest-small/s3/')\n",
    "\n",
    "# one-hot encode the testing data for model input\n",
    "with tf.Session() as tf_sess:\n",
    "    test_user_data = tf_sess.run(tf.one_hot(user_test, depth=n_user)).tolist()\n",
    "    test_item_data = tf_sess.run(tf.one_hot(item_test, depth=n_item)).tolist()\n",
    "\n",
    "# if you're using Tensorflow 2.0 for one hot encoding\n",
    "# you can convert the tensor to list using:\n",
    "# tf.one_hot(uuser_test, depth=n_user).numpy().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make batch prediction\n",
    "batch_size = 100\n",
    "y_pred = []\n",
    "for idx in range(0, len(test_user_data), batch_size):\n",
    "    # reformat test samples into tensorflow serving acceptable format\n",
    "    input_vals = {\n",
    "     \"instances\": [\n",
    "         {'input_1': u, 'input_2': i}\n",
    "         for (u, i) in zip(test_user_data[idx:idx+batch_size], test_item_data[idx:idx+batch_size])\n",
    "    ]}\n",
    "\n",
    "    # invoke model endpoint to make inference\n",
    "    pred = predictor.predict(input_vals)\n",
    "\n",
    "    # store predictions\n",
    "    y_pred.extend([i[0] for i in pred['predictions']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's see some prediction examples, assuming the threshold\n",
    "# --- prediction probability view ---\n",
    "print('This is what the prediction output looks like')\n",
    "print(y_pred[:5], end='\\n\\n\\n')\n",
    "\n",
    "# --- user item pair prediction view, with threshold of 0.5 applied ---\n",
    "pred_df = pd.DataFrame([\n",
    "    user_test,\n",
    "    item_test,\n",
    "    (np.array(y_pred) >= 0.5).astype(int)],\n",
    ").T\n",
    "\n",
    "pred_df.columns = ['userId', 'movieId', 'prediction']\n",
    "\n",
    "print('We can convert the output to user-item pair as shown below')\n",
    "print(pred_df.head(), end='\\n\\n\\n')\n",
    "\n",
    "# --- aggregated prediction view, by user ---\n",
    "print('Lastly, we can roll up the prediction list by user and view it that way')\n",
    "print(pred_df.query('prediction == 1').groupby('userId').movieId.apply(list).head().to_frame(), end='\\n\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Delete Endpoint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete endpoint at the end of the demo\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}